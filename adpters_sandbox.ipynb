{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a72640-8a99-4895-9abb-c19baa99a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91c7f75c-2237-4d0b-a250-27dbe110560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Union\n",
    "\n",
    "class RifeADAPTER:\n",
    "    def __init__(\n",
    "        self,\n",
    "        img: Sequence[Union[str, Path]],\n",
    "        exp: int = 4,\n",
    "        ratio: float = 0.0,\n",
    "        rthreshold: float = 0.02,\n",
    "        rmaxcyc: int = 8,\n",
    "        model: Union[str, Path] = \"ECCV2022-RIFE/train_log\",\n",
    "        out_dir: Union[str, Path] = \"./output\"\n",
    "    ):\n",
    "        self.img = [str(Path(p)) for p in img]\n",
    "        self.exp = int(exp)\n",
    "        self.ratio = float(ratio) # inference ratio between two images with 0 - 1 range\n",
    "        self.rthreshold = float(rthreshold) # returns image when actual ratio falls in given range threshold\n",
    "        self.rmaxcyc = int(rmaxcyc) # limit max number of bisectional cycles\n",
    "        self.model = str(model) # directory with trained model files\n",
    "        self.out_dir = str(out_dir)\n",
    "\n",
    "    def _build_cmd(self) -> list[str]:\n",
    "        return [\n",
    "            \"python3\", \"ECCV2022-RIFE/inference_img.py\",\n",
    "            \"--img\", *self.img,\n",
    "            \"--exp\", str(self.exp),\n",
    "            \"--ratio\", str(self.ratio),\n",
    "            \"--rthreshold\", str(self.rthreshold),\n",
    "            \"--rmaxcycles\", str(self.rmaxcyc),\n",
    "            \"--model\", self.model,\n",
    "        ]\n",
    "\n",
    "    def _build_ffmpeg_cmd(\n",
    "        self,\n",
    "        pattern: str,\n",
    "        gif_path: str,\n",
    "        framerate: int,\n",
    "        size: str,\n",
    "    ) -> list[str]:\n",
    "        return [\n",
    "            \"ffmpeg\",\n",
    "            \"-y\",                       # перезаписывать без подтверждения\n",
    "            \"-r\", str(framerate),\n",
    "            \"-f\", \"image2\",\n",
    "            \"-i\", pattern,              # например output/img%d.png\n",
    "            \"-s\", size,   \n",
    "            \"-vf\",\n",
    "            \"split[s0][s1];[s0]palettegen=stats_mode=single[p];\"\n",
    "            \"[s1][p]paletteuse=new=1\",\n",
    "            gif_path,\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, **subprocess_kwargs):\n",
    "        cmd = self._build_cmd()\n",
    "        print(\"Executing:\", shlex.join(cmd))\n",
    "        return subprocess.run(cmd, check=True, **subprocess_kwargs)\n",
    "\n",
    "\n",
    "    def create_gif(\n",
    "        self,\n",
    "        framerate: int = 10,\n",
    "        size: str = \"448x256\",\n",
    "        pattern: str | None = None,\n",
    "        gif_name: str = \"slomo.gif\",\n",
    "        **subprocess_kwargs,\n",
    "    ):\n",
    "        pattern = pattern or f\"{self.out_dir}/img%d.png\"\n",
    "        gif_path = gif_name if Path(gif_name).is_absolute() else f\"{self.out_dir}/{gif_name}\"\n",
    "        Path(gif_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cmd = self._build_ffmpeg_cmd(pattern, gif_path, framerate, size)\n",
    "        print(\"Executing:\", shlex.join(cmd))\n",
    "        return subprocess.run(cmd, check=True, **subprocess_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ad2dcfa-7855-4745-8969-9ed1b4314549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python3 ECCV2022-RIFE/inference_img.py --img ECCV2022-RIFE/demo/I0_0.png ECCV2022-RIFE/demo/I0_1.png --exp 4 --ratio 0.0 --rthreshold 0.02 --rmaxcycles 8 --model ECCV2022-RIFE/train_log\n",
      "Loaded v3.x HD model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adapter = RifeADAPTER(img=['ECCV2022-RIFE/demo/I0_0.png', 'ECCV2022-RIFE/demo/I0_1.png'])\n",
    "result = adapter(capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "096aa0f0-e00e-457a-bb8f-4a89af10c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: ffmpeg -y -r 12 -f image2 -i ./output/img%d.png -s 512x288 -vf 'split[s0][s1];[s0]palettegen=stats_mode=single[p];[s1][p]paletteuse=new=1' ./output/slomo.gif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from './output/img%d.png':\n",
      "  Duration: 00:00:01.42, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc), 448x256, 12 fps, 12 tbr, 12 tbn, 12 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> gif (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 21797 colors; ratio=0.011699\n",
      "Output #0, gif, to './output/slomo.gif':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: gif, pal8(pc, progressive), 512x288, q=2-31, 200 kb/s, 12 fps, 100 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 gif\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 36566 colors; ratio=0.006974\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 38304 colors; ratio=0.006657\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 39580 colors; ratio=0.006443\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 40328 colors; ratio=0.006323\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 40883 colors; ratio=0.006237\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41368 colors; ratio=0.006164\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41447 colors; ratio=0.006152\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41850 colors; ratio=0.006093\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41591 colors; ratio=0.006131\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41426 colors; ratio=0.006156\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 41120 colors; ratio=0.006201\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 40688 colors; ratio=0.006267\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 39812 colors; ratio=0.006405\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 38647 colors; ratio=0.006598\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 37120 colors; ratio=0.006870\n",
      "[Parsed_palettegen_1 @ 0x642749e76980] 255(+1) colors generated out of 22000 colors; ratio=0.011591\n",
      "frame=   17 fps=0.0 q=-0.0 Lsize=    1188kB time=00:00:01.34 bitrate=7261.1kbits/s speed=1.43x    \n",
      "video:1188kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.001644%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ffmpeg', '-y', '-r', '12', '-f', 'image2', '-i', './output/img%d.png', '-s', '512x288', '-vf', 'split[s0][s1];[s0]palettegen=stats_mode=single[p];[s1][p]paletteuse=new=1', './output/slomo.gif'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter.create_gif(framerate=12, size=\"512x288\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8abc42-ec4e-4c5e-99b4-4b8506d4b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class tpsAdapter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image1: Union[str, Path] = \"tps-inbetween/assets/input1_0.png\",\n",
    "        image2: Union[str, Path] = \"tps-inbetween/assets/input1_1.png\",\n",
    "        xN: int = 30,\n",
    "        save_path: Union[str, Path] = \"./output\",\n",
    "        cpu: bool = False,\n",
    "        model_path: Union[str, Path] = \"tps-inbetween/ckpt/model_latest.pt\",\n",
    "    ):\n",
    "        self.image1 = str(Path(image1))\n",
    "        self.image2 = str(Path(image2))\n",
    "        self.xN = int(xN)\n",
    "        self.save_path = str(Path(save_path))\n",
    "        self.cpu = bool(cpu)\n",
    "        self.model_path = str(Path(model_path))\n",
    "\n",
    "    def _build_cmd(self) -> list[str]:\n",
    "        cmd = [\n",
    "            \"python\", \"tps-inbetween/demo.py\",\n",
    "            \"--image1\", self.image1,\n",
    "            \"--image2\", self.image2,\n",
    "            \"--xN\", str(self.xN),\n",
    "            \"--save_path\", self.save_path,\n",
    "            \"--model_path\", self.model_path,\n",
    "        ]\n",
    "        if self.cpu:\n",
    "            cmd.append(\"--cpu\")\n",
    "        return cmd\n",
    "\n",
    "    def __call__(self, **subprocess_kwargs):\n",
    "        cmd = self._build_cmd()\n",
    "        print(\"Executing:\", shlex.join(cmd))\n",
    "        return subprocess.run(cmd, check=True, **subprocess_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c4a8ac-9664-46a8-99a9-86e5416ab8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python tps-inbetween/demo.py --image1 tps-inbetween/assets/input1_0.png --image2 tps-inbetween/assets/input1_1.png --xN 30 --save_path output --model_path tps-inbetween/ckpt/model_latest.pt\n",
      "======= Start Loading Matching Model======\n",
      "Successfully loading matching model! Start matching ...\n",
      "Matching complete! Results have saved to  output/matching.png\n",
      "====== Start Inbetweening ======\n",
      "Finish! Results have saved to  output/out.gif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adapter = tpsAdapter(\n",
    "    image1='tps-inbetween/assets/input1_0.png',\n",
    "    image2='tps-inbetween/assets/input1_1.png',\n",
    "    xN=30,\n",
    "    save_path='./output',\n",
    "    cpu=False,                                   \n",
    "    model_path='tps-inbetween/ckpt/model_latest.pt',\n",
    ")\n",
    "\n",
    "result = adapter(capture_output=True, text=True)\n",
    "print(result.stdout)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d39f0a-4a79-42dd-a50f-f57821892bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class sainAdapter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        frameA: Union[str, Path],\n",
    "        frameB: Union[str, Path],\n",
    "        out: Union[str, Path] = \"SAIN/inbetween.png\",\n",
    "        ckpt: Union[str, Path] = \"SAIN/checkpoint/model_best.pth\",\n",
    "        size: int = 512,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        if size % 8 != 0:\n",
    "            raise ValueError(\"--size должно быть кратно 8\")\n",
    "\n",
    "        self.frameA = str(Path(frameA))\n",
    "        self.frameB = str(Path(frameB))\n",
    "        self.out = str(Path(out))\n",
    "        self.ckpt = str(Path(ckpt))\n",
    "        self.size = int(size)\n",
    "        self.device = device.lower()  # 'cuda' или 'cpu'\n",
    "\n",
    "    def _build_cmd(self) -> list[str]:\n",
    "        return [\n",
    "            \"python\", \"SAIN/infer.py\",\n",
    "            \"--frameA\", self.frameA,\n",
    "            \"--frameB\", self.frameB,\n",
    "            \"--out\", self.out,\n",
    "            \"--ckpt\", self.ckpt,\n",
    "            \"--size\", str(self.size),\n",
    "            \"--device\", self.device,\n",
    "        ]\n",
    "\n",
    "    def __call__(self, **subprocess_kwargs):\n",
    "        cmd = self._build_cmd()\n",
    "        print(\"Executing:\", shlex.join(cmd))\n",
    "        return subprocess.run(cmd, check=True, **subprocess_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459ba209-8701-4eba-8b04-dee65c667439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: python SAIN/infer.py --frameA SAIN/frame1.png --frameB SAIN/frame2.png --out result.png --ckpt SAIN/checkpoint/model_best.pth --size 512 --device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ saved → result.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'SAIN/infer.py', '--frameA', 'SAIN/frame1.png', '--frameB', 'SAIN/frame2.png', '--out', 'result.png', '--ckpt', 'SAIN/checkpoint/model_best.pth', '--size', '512', '--device', 'cuda'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter = sainAdapter(\n",
    "    frameA=\"SAIN/frame1.png\",\n",
    "    frameB=\"SAIN/frame2.png\",\n",
    "    out=\"result.png\",\n",
    "    size=512,           \n",
    "    device=\"cuda\"       \n",
    ")\n",
    "\n",
    "adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98912e-90df-4f55-a4f3-6f206910143d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb5416-c81a-46bf-bc30-ec2df1a9fc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaaebb3-dfb9-4ae9-995c-9333a2d09997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bfbe48-04f7-446d-9bde-edf361359d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rife import RifeAdapter\n",
    "from tps  import TpsAdapter\n",
    "from sain import SainAdapter\n",
    "\n",
    "\n",
    "ADAPTERS = {\n",
    "    \"rife\": RifeAdapter(),    \n",
    "    \"tps\" : TpsAdapter(),\n",
    "    \"sain\": SainAdapter(),\n",
    "}\n",
    "\n",
    "def run_all(img1: str, img2: str, *, verbose=True, **subproc_kw):\n",
    "    results = {}\n",
    "    for name, adapter in ADAPTERS.items():\n",
    "        if verbose:\n",
    "            print(f\"\\n=== {name.upper()} ===\")\n",
    "        results[name] = adapter(img1, img2, **subproc_kw)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a257b99-c5be-49d7-b46b-4bc173c28c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RIFE ===\n",
      "Executing: python3 ECCV2022-RIFE/inference_img.py --img SAIN/frame1.png SAIN/frame2.png --exp 4 --ratio 0.0 --rthreshold 0.02 --rmaxcycles 8 --model ECCV2022-RIFE/train_log\n",
      "Loaded v3.x HD model.\n",
      "\n",
      "=== TPS ===\n",
      "Executing: python tps-inbetween/demo.py --image1 SAIN/frame1.png --image2 SAIN/frame2.png --xN 30 --save_path ./output --model_path tps-inbetween/ckpt/model_latest.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/ADAPTERS/tps-inbetween/model/gluestick/models/gluestick.py:346: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Start Loading Matching Model======\n",
      "Successfully loading matching model! Start matching ...\n",
      "Matching complete! Results have saved to  ./output/matching.png\n",
      "====== Start Inbetweening ======\n",
      "Finish! Results have saved to  ./output/out.gif\n",
      "\n",
      "=== SAIN ===\n",
      "Executing: python SAIN/infer.py --frameA SAIN/frame1.png --frameB SAIN/frame2.png --out output/result_frame1_frame2_sain.png --ckpt SAIN/checkpoint/model_best.pth --size 512 --device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ saved → output/result_frame1_frame2_sain.png\n",
      "\n",
      "--- rife stdout ---\n",
      "None\n",
      "\n",
      "--- tps stdout ---\n",
      "None\n",
      "\n",
      "--- sain stdout ---\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "imgA = \"SAIN/frame1.png\"\n",
    "imgB = \"SAIN/frame2.png\"\n",
    "\n",
    "logs = run_all(imgA, imgB, capture_output=True, text=True)\n",
    "\n",
    "for name, proc in logs.items():\n",
    "    print(f\"\\n--- {name} stdout ---\")\n",
    "    print(proc.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5579223-753f-4fd2-8e36-07dda56293cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
